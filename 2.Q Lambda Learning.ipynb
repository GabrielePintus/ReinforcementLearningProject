{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lunar Lander with Q Learning\n",
    "\n",
    "In this notebook we are going to train an agent to play the Lunar Lander game using tabular methods as:\n",
    "- Q Learning\n",
    "- Q Learning with Eligibility Traces\n",
    "- Q Learning with Eligibility Traces and spatial decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Video display\n",
    "from IPython.display import Video\n",
    "from moviepy import *\n",
    "\n",
    "# Custom modules\n",
    "from src.utils import *\n",
    "\n",
    "# Set seed\n",
    "SEED = 31\n",
    "np.random.seed(SEED)\n",
    "print(f\"Using seed {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment\n",
    "\n",
    "The Lunar Lander environment is a 2D environment where the agent has to land a spaceship on a landing pad.\n",
    "The agent has 4 actions available:\n",
    "- Do nothing\n",
    "- Fire left orientation engine\n",
    "- Fire main engine\n",
    "- Fire right orientation engine\n",
    "\n",
    "The agent receives a reward of 100 for landing on the landing pad and -100 for crashing. The agent also receives a reward proportional to the fuel used to land the spaceship.\n",
    "\n",
    "The state space is a 8-dimensional vector with the following components:\n",
    "- x position\n",
    "- y position\n",
    "- x velocity\n",
    "- y velocity\n",
    "- angle\n",
    "- angular velocity\n",
    "- left leg contact\n",
    "- right leg contact\n",
    "\n",
    "The environment is considered solved when the agent reaches an average reward of 200 over 100 episodes.\n",
    "\n",
    "![Lunar Lander](https://www.gymlibrary.dev/_images/lunar_lander.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env_name = 'LunarLander-v3'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The discretization\n",
    "\n",
    "In order to utilize the first three algorithms we need to discretize the observation space. We are going to use a simple discretization method where we divide the observation space into a grid of cells. Each cell is going to be represented by a state in the Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.discretization import DiscretizeObservationWrapper\n",
    "\n",
    "N = 20\n",
    "n_bins = np.array([N, N, N, N, N, N, 2, 2])\n",
    "env = DiscretizeObservationWrapper(env, n_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Q_agents import QLambdaAgentME as QLambdaAgent\n",
    "\n",
    "n_episodes = 20_000\n",
    "horizon = 2_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'env': env,\n",
    "    'discount_factor': 0.99,\n",
    "    'initial_epsilon': 1.0,\n",
    "    'epsilon_decay': 0.9997,\n",
    "    'min_epsilon': 0.01,\n",
    "    'learning_rate': 0.5,\n",
    "    'seed': SEED,\n",
    "    'trace_decay': 0.9,\n",
    "}\n",
    "\n",
    "agent = QLambdaAgent(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_q, total_steps = agent.learn(n_episodes, horizon)\n",
    "rewards_q = pd.Series(rewards_q)\n",
    "\n",
    "print(f\"Total steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(rewards_q, title=\"Q Agent\", sma=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = Path('./gym_videos')\n",
    "video_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "env = gym.make(env_name, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=video_dir,\n",
    "    name_prefix=env_name,\n",
    ")\n",
    "env = DiscretizeObservationWrapper(env, n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, rewards = play(agent, env, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, episode_frames in enumerate(frames):\n",
    "    filename = f\"{video_dir}/{env_name}_{i}.mp4\"\n",
    "    clip = ImageSequenceClip(episode_frames, fps=60)\n",
    "    clip.write_videofile(filename, codec='libx264')\n",
    "    \n",
    "    print(f\"Episode {i} reward: {rewards[i]:.2f}\")    \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = [ x for x in list(video_dir.glob('*.mp4')) if env_name in x.name]\n",
    "\n",
    "for video in videos:\n",
    "    print(video)\n",
    "    display(Video(video))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
