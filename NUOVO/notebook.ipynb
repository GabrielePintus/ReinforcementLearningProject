{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Custom modules\n",
    "from environment import EnvironmentState\n",
    "from agents import LearningAgent\n",
    "from data import DataGenerator\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_levels = 10\n",
    "horizon = 20\n",
    "action_size = 10\n",
    "n_episodes = 1\n",
    "\n",
    "data = DataGenerator.generator(\"../data/AAPL.parquet\", levels=n_levels).head(30000)\n",
    "\n",
    "means = data[['Mid Price Movement', 'Market Spread', 'Book Imbalance', 'Signed Volume', 'Volatility', 'RSI']].mean()\n",
    "means = np.concatenate([means.values, [0, 2.5, 2.5, 4.5]])\n",
    "\n",
    "stds = data[['Mid Price Movement', 'Market Spread', 'Book Imbalance', 'Signed Volume', 'Volatility', 'RSI']].std()\n",
    "stds = np.concatenate([stds.values, [1e4/3, 1.9, 1.9, 3]])\n",
    "\n",
    "# Combine the two vectors in a matirx\n",
    "std_values = np.vstack((means, stds)).T.astype(np.float32)\n",
    "\n",
    "windows = []\n",
    "for i in range(n_episodes):\n",
    "    windows.append(data.iloc[i*(horizon+1):(i+1)*(horizon+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_values = data[['Mid Price Movement', 'Market Spread', 'Book Imbalance', 'Signed Volume', 'Volatility', 'RSI']].min().values\n",
    "max_values = data[['Mid Price Movement', 'Market Spread', 'Book Imbalance', 'Signed Volume', 'Volatility', 'RSI']].max().values\n",
    "\n",
    "min_values = np.concatenate([min_values, [-1e4, -1, -1, 0]])\n",
    "max_values = np.concatenate([max_values, [1e4, 5, 5, 9]])\n",
    "\n",
    "bounds = np.vstack((min_values, max_values)).T.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MLP(10, 1, [64, 128, 64], torch.nn.ReLU, 0.05, device).to(device)\n",
    "# q_approximator = QValueApproximator(model, 1e-4, std_values)\n",
    "\n",
    "from approximators import TilingApproximatorMedium\n",
    "\n",
    "q_approximator = TilingApproximatorMedium(\n",
    "    bounds = bounds,\n",
    "    n_tiles = 16,\n",
    "    n_tilings = 64,\n",
    "    # We can sample random shifts for each dimension\n",
    "    shifts = np.random.uniform(0.8, 0.19, 10),\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import Logger\n",
    "\n",
    "logger = Logger()\n",
    "agent = LearningAgent(\n",
    "    action_size=action_size,\n",
    "    q_value_approximator=q_approximator,\n",
    "    epsilon=0.5,\n",
    "    epsilon_decay=0.97,\n",
    "    min_epsilon=0.01,\n",
    "    gamma=0.99,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "\n",
    "from LOB import LOB\n",
    "\n",
    "limit_order_book = LOB(data, n_levels)\n",
    "\n",
    "env = EnvironmentState(limit_order_book, n_levels, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd94e68b675f4c61b2daf10bfc2195aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards_train, losses_train, bankrolls_train = [], [], []\n",
    "outstanding_orders = []\n",
    "inventory = []\n",
    "\n",
    "for episode in tqdm(range(1*n_episodes)):\n",
    "    episode = episode % n_episodes\n",
    "    env.data = windows[episode]\n",
    "    env.reset()\n",
    "    rewards, losses, bankrolls = agent.train(env)\n",
    "\n",
    "    # Track the evolution of the number of outstanding orders\n",
    "    outstanding_orders.append(len(env.orders))\n",
    "    inventory.append(env.inventory)\n",
    "    \n",
    "    # rewards_train.extend(rewards)\n",
    "    # losses_train.extend(losses)\n",
    "    # bankrolls_train.extend(bankrolls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
