{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Making with Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generic\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Reinforcement Learning\n",
    "import gym\n",
    "\n",
    "# Custom Modules\n",
    "from Libraries import environment as env\n",
    "from Libraries import agents as ag\n",
    "from Libraries import approximators as val_approx\n",
    "from data.data_gen import DataGenerator\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the training environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob_data = DataGenerator._generator('data/lob.csv', levels=1)\n",
    "lob_data = lob_data.head(1200).values\n",
    "\n",
    "# Set the boundaries for the state space\n",
    "boundaries = np.array([lob_data.min(axis=0), lob_data.max(axis=0)]).T * np.array([0.8, 1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.23990000e+02,  1.00000000e+02,  2.23750000e+02,  7.40000000e+01,\n",
       "        2.23870000e+02,  2.40000000e-01, -5.00000000e-03, -1.49425287e-01,\n",
       "       -2.60000000e+01, -2.23338917e-05,  2.40100065e-05,  4.16666667e+01])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = env.MarketMakerEnv(\n",
    "    lob_data=lob_data,\n",
    "    horizon=1000,\n",
    "    phi_transorm=env.PhiTransform.PnL_asymm_dampened(-0.5),\n",
    ")\n",
    "initial_state = env.reset()\n",
    "display(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58911fa670946ada42e043d8d521f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBSERVATION SPACE:  [ 2.23990000e+02  1.00000000e+02  2.23750000e+02  7.40000000e+01\n",
      "  2.23870000e+02  2.40000000e-01  0.00000000e+00 -1.49425287e-01\n",
      " -2.60000000e+01  0.00000000e+00  2.40100065e-05  4.16666667e+01]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(223.99, 100.0, 223.75, 74.0, 223.87, 0.2400000000000091, -0.0049999999999954525, -0.14942528735632185, -26.0, -2.2333891680603735e-05, 2.4010006537778737e-05, 41.66666666667456)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m\n\u001b[0;32m      2\u001b[0m agent \u001b[38;5;241m=\u001b[39m ag\u001b[38;5;241m.\u001b[39mQLambdaAgent(\n\u001b[0;32m      3\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m      4\u001b[0m     value_function\u001b[38;5;241m=\u001b[39mval_approx\u001b[38;5;241m.\u001b[39mTileCodingValueFunction(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     el_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m train_rewards \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain(n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\marta\\OneDrive\\Desktop\\ReinforcementLearningProject\\Libraries\\agents.py:65\u001b[0m, in \u001b[0;36mLearningAgent.train\u001b[1;34m(self, n_episodes)\u001b[0m\n\u001b[0;32m     62\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     63\u001b[0m next_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoose_action(next_state)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn(state, action, reward, next_state, next_action, done)\n\u001b[0;32m     67\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     68\u001b[0m action \u001b[38;5;241m=\u001b[39m next_action\n",
      "File \u001b[1;32mc:\\Users\\marta\\OneDrive\\Desktop\\ReinforcementLearningProject\\Libraries\\agents.py:34\u001b[0m, in \u001b[0;36mLearningAgent.learn\u001b[1;34m(self, state, action, reward, next_state, next_action, done)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, reward, next_state, next_action, done):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mel_decay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_rule(state, action, reward, next_state, next_action, done, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_function, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mel_decay, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meligibility_trace)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_rule(state, action, reward, next_state, next_action, done, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_function, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma)\n",
      "File \u001b[1;32mc:\\Users\\marta\\OneDrive\\Desktop\\ReinforcementLearningProject\\Libraries\\agents.py:201\u001b[0m, in \u001b[0;36mLearningUpdates.q_lambda_update\u001b[1;34m(state, action, reward, next_state, next_action, done, value_function, alpha, gamma, el_decay, eligibility_trace)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (state_tuple, action) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m eligibility_trace:\n\u001b[0;32m    199\u001b[0m     eligibility_trace[state_tuple, action] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 201\u001b[0m eligibility_trace[state_tuple][action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Iterate over all the state-action pairs in the eligibility trace\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (s, a), trace_value \u001b[38;5;129;01min\u001b[39;00m eligibility_trace\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;66;03m# Update the value function for each (state, action) pair\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: (223.99, 100.0, 223.75, 74.0, 223.87, 0.2400000000000091, -0.0049999999999954525, -0.14942528735632185, -26.0, -2.2333891680603735e-05, 2.4010006537778737e-05, 41.66666666667456)"
     ]
    }
   ],
   "source": [
    "# Define the agent\n",
    "agent = ag.QLambdaAgent(\n",
    "    env=env,\n",
    "    value_function=val_approx.TileCodingValueFunction(\n",
    "        num_tiles=16,\n",
    "        num_tilings=32,\n",
    "        state_bounds=boundaries,\n",
    "        action_space_n=env.ACTIONS_0_8.shape[0],\n",
    "    ),\n",
    "    alpha=1e-3,\n",
    "    gamma=0.7,\n",
    "    epsilon=0.1,\n",
    "    epsilon_decay=0.995, \n",
    "    epsilon_min=1e-2,\n",
    "    el_decay = 0.8\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "train_rewards = agent.train(n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rewards_SMAs = pd.Series(train_rewards).rolling(window=10).mean()\n",
    "\n",
    "plt.plot(train_rewards, alpha=0.5)\n",
    "plt.plot(train_rewards_SMAs, color='red')\n",
    "plt.title('Training Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "test_rewards = agent.test(n_episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth the rewards\n",
    "sma_window = 10\n",
    "rewards_sma = pd.Series(test_rewards).rolling(window=sma_window).mean()\n",
    "\n",
    "# Plot the rewards\n",
    "plt.plot(test_rewards, alpha=0.5)\n",
    "plt.plot(rewards_sma, color='red')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
