{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander with Q Learning\n",
    "\n",
    "In this notebook we are going to train an agent to play the Lunar Lander game using tabular methods as:\n",
    "- Q Learning\n",
    "- Q Learning with Eligibility Traces\n",
    "- Q Learning with Eligibility Traces and spatial decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Video display\n",
    "from IPython.display import Video\n",
    "from moviepy import *\n",
    "\n",
    "# Custom modules\n",
    "from src.utils import *\n",
    "\n",
    "\n",
    "# Set seed\n",
    "SEED = 31\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "print(f\"Using seed {SEED}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment\n",
    "\n",
    "The Lunar Lander environment is a 2D environment where the agent has to land a spaceship on a landing pad.\n",
    "The agent has 4 actions available:\n",
    "- Do nothing\n",
    "- Fire left orientation engine\n",
    "- Fire main engine\n",
    "- Fire right orientation engine\n",
    "\n",
    "The agent receives a reward of 100 for landing on the landing pad and -100 for crashing. The agent also receives a reward proportional to the fuel used to land the spaceship.\n",
    "\n",
    "The state space is a 8-dimensional vector with the following components:\n",
    "- x position\n",
    "- y position\n",
    "- x velocity\n",
    "- y velocity\n",
    "- angle\n",
    "- angular velocity\n",
    "- left leg contact\n",
    "- right leg contact\n",
    "\n",
    "The environment is considered solved when the agent reaches an average reward of 200 over 100 episodes.\n",
    "\n",
    "![Lunar Lander](https://www.gymlibrary.dev/_images/lunar_lander.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env_name = 'LunarLander-v3'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.networks import ActorNet, CriticNet\n",
    "\n",
    "n_episodes = 2_000\n",
    "\n",
    "initial_lr_actor = 1e-3 * 1.2\n",
    "final_lr_actor = 1e-3 * 1.2\n",
    "\n",
    "initial_lr_critic = 5e-3 * 1.2\n",
    "final_lr_critic = 5e-4 * 1.2\n",
    "# gamma_actor = np.log(final_lr_actor / initial_lr_actor) / n_episodes\n",
    "# gamma_critic = np.log(final_lr_critic / initial_lr_critic) / n_episodes\n",
    "gamma_actor = 1\n",
    "gamma_critic = 1\n",
    "\n",
    "actor_net_main = ActorNet(\n",
    "    input_dim=8,\n",
    "    output_dim=4,\n",
    "    hidden_dim=[64,64],\n",
    "    batchnorm=False,\n",
    "    activation=nn.ReLU,\n",
    "    dropout=0.2,\n",
    "    device=device,\n",
    "    lr_scheduler_params={\n",
    "        'gamma': gamma_actor\n",
    "    },\n",
    "    optimizer_params={\n",
    "        \"lr\": initial_lr_actor,\n",
    "        \"weight_decay\": 1e-8,\n",
    "        \"betas\": (0.9, 0.99),\n",
    "    },\n",
    "    output_activation=nn.Softmax\n",
    ")\n",
    "\n",
    "critic_net_main = CriticNet(\n",
    "    input_dim=8,\n",
    "    hidden_dim=[64,64],\n",
    "    output_dim=1,\n",
    "    batchnorm=False,\n",
    "    activation=nn.ReLU,\n",
    "    dropout=0.1,\n",
    "    device=device,\n",
    "    lr_scheduler_params={\n",
    "        'gamma': gamma_critic\n",
    "    },\n",
    "    optimizer_params={\n",
    "        \"lr\": initial_lr_critic,\n",
    "        \"weight_decay\": 1e-8,\n",
    "        \"betas\": (0.9, 0.99),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DeepAgents import MonteCarloActorCritic as MCAC\n",
    "\n",
    "agent = MCAC(\n",
    "    env = env,\n",
    "    discount_factor = 0.99,\n",
    "    policy_net=actor_net_main,\n",
    "    value_net=critic_net_main,\n",
    "    inertia=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, losses_actor, losses_critic = agent.learn(n_episodes, 2_000)\n",
    "\n",
    "rewards = pd.Series(rewards)\n",
    "losses_actor = pd.Series(losses_actor)\n",
    "losses_critic = pd.Series(losses_critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the rewards obtained by the agent during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(rewards.rolling(20).mean(), label=\"Reward\")\n",
    "ax[0].set_title(\"Reward\")\n",
    "ax[0].set_xlabel(\"Episode\")\n",
    "ax[0].set_ylabel(\"Reward\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(losses_actor.rolling(20).mean(), label=\"Actor Loss\")\n",
    "ax[1].set_title(\"Loss\")\n",
    "ax[1].set_xlabel(\"Episode\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(losses_critic.rolling(20).mean(), label=\"Critic Loss\")\n",
    "ax[2].set_title(\"Loss\")\n",
    "ax[2].set_xlabel(\"Episode\")\n",
    "ax[2].set_ylabel(\"Loss\")\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate video of the agent playing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = Path('./gym_videos')\n",
    "video_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "env = gym.make(env_name, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    video_folder=video_dir,\n",
    "    name_prefix=env_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, rewards = play(agent, env, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, episode_frames in enumerate(frames):\n",
    "    filename = f\"{video_dir}/{env_name}_{i}.mp4\"\n",
    "    clip = ImageSequenceClip(episode_frames, fps=60)\n",
    "    clip.write_videofile(filename, codec='libx264')\n",
    "    \n",
    "    print(f\"Episode {i} reward: {rewards[i]:.2f}\")    \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = [ x for x in list(video_dir.glob('*.mp4')) if env_name in x.name]\n",
    "\n",
    "for video in videos:\n",
    "    print(video)\n",
    "    display(Video(video))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
