{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Making with Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generic\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Reinforcement Learning\n",
    "import gym\n",
    "\n",
    "# Custom Modules\n",
    "from Environment.market_making import MarketMakerEnv, PhiTransform\n",
    "from data.data_gen import DataGenerator\n",
    "import algorithms\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the training environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob_data = DataGenerator._generator('data/lob.csv', levels=1)\n",
    "lob_data = lob_data.head(1200).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.23990000e+02,  1.00000000e+02,  2.23750000e+02,  7.40000000e+01,\n",
       "        2.23870000e+02,  2.40000000e-01, -5.00000000e-03, -1.49425287e-01,\n",
       "       -2.60000000e+01, -2.23338917e-05,  2.40100065e-05,  4.16666667e+01])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = MarketMakerEnv(\n",
    "    lob_data=lob_data,\n",
    "    horizon=1000,\n",
    "    phi_transorm=PhiTransform.PnL_asymm_dampened(-1),\n",
    ")\n",
    "initial_state = env.reset()\n",
    "display(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02dbabe9c54e4ed4a8b45470a020e218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/200 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m      2\u001b[0m agent \u001b[38;5;241m=\u001b[39m algorithms\u001b[38;5;241m.\u001b[39mQLearningAgent(\n\u001b[1;32m      3\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# value_function=algorithms.TileCodingValueFunction(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     epsilon_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documenti/Progetti/ReinforcementLearningProject/algorithms.py:59\u001b[0m, in \u001b[0;36mQLearningAgent.train\u001b[0;34m(self, n_episodes)\u001b[0m\n\u001b[1;32m     56\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[1;32m     57\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     62\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/Documenti/Progetti/ReinforcementLearningProject/algorithms.py:39\u001b[0m, in \u001b[0;36mQLearningAgent.learn\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     36\u001b[0m td_target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m next_q_values[best_next_action] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;28;01melse\u001b[39;00m reward)\n\u001b[1;32m     37\u001b[0m td_error \u001b[38;5;241m=\u001b[39m td_target \u001b[38;5;241m-\u001b[39m q_values[action]\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtd_error\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documenti/Progetti/ReinforcementLearningProject/algorithms.py:138\u001b[0m, in \u001b[0;36mLinearFunctionApproximator.update\u001b[0;34m(self, state, action, td_error)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, td_error):\n\u001b[0;32m--> 138\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m td_error \u001b[38;5;241m*\u001b[39m features\n",
      "File \u001b[0;32m~/Documenti/Progetti/ReinforcementLearningProject/algorithms.py:130\u001b[0m, in \u001b[0;36mLinearFunctionApproximator.get_features\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[1;32m    129\u001b[0m     features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 130\u001b[0m     \u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Example: Simple encoding, adjust based on your state representation\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# Define the agent\n",
    "agent = algorithms.QLearningAgent(\n",
    "    env=env,\n",
    "    # value_function=algorithms.TileCodingValueFunction(\n",
    "    #     num_tiles=10,\n",
    "    #     num_tilings=4,\n",
    "    #     state_bounds=(-1,1),\n",
    "    #     action_space_n=env.ACTIONS_0_8.shape[0],\n",
    "    # ),\n",
    "    alpha=1e-3,\n",
    "    gamma=0.99,\n",
    "    epsilon=0.1,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=1e-2,\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "agent.train(n_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "rewards = agent.test(n_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth the rewards\n",
    "sma_window = 10\n",
    "rewards_sma = pd.Series(rewards).rolling(window=sma_window).mean()\n",
    "\n",
    "# Plot the rewards\n",
    "# plt.plot(rewards, label='Reward')\n",
    "plt.plot(rewards_sma, label='SMA 10 Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
